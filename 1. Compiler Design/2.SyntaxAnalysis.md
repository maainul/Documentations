# 2. Syntax Analysis
```
Syntax analysis or parsing is the second phase of a compiler. 
In this chapter, we shall learn the basic concepts used in the construction of a parser.
```
```
We have seen that a lexical analyzer can identify tokens with the help of regular expressions and pattern rules. 
But a lexical analyzer cannot check the syntax of a given sentence due to the limitations of the regular expressions. 
Regular expressions cannot check balancing tokens, such as parenthesis. 
Therefore, this phase uses context-free grammar (CFG), which is recognized by push-down automata.
```
```
CFG, on the other hand, is a superset of Regular Grammar, as depicted below:
```
            ![syntaxanalysis](https://user-images.githubusercontent.com/37740006/75222801-33600900-57cf-11ea-8416-cf6d0c2e162c.png)

# Relation of CFG and Regular Grammar
```
It implies that every Regular Grammar is also context-free, 
but there exists some problems, which are beyond the scope of Regular Grammar. 
CFG is a helpful tool in describing the syntax of programming languages.
```
# Context-Free Grammar
```
In this section, we will first see the definition of context-free grammar and introduce terminologies used in parsing technology.
```

A context-free grammar has four components:
```
A set of non-terminals (V). Non-terminals are syntactic variables that denote sets of strings. The non-terminals define sets of strings that help define the language generated by the grammar.

A set of tokens, known as terminal symbols (Σ). Terminals are the basic symbols from which strings are formed.

A set of productions (P). The productions of a grammar specify the manner in which the terminals and non-terminals can be combined to form strings. Each production consists of a non-terminal called the left side of the production, an arrow, and a sequence of tokens and/or on- terminals, called the right side of the production.

One of the non-terminals is designated as the start symbol (S); from where the production begins.

The strings are derived from the start symbol by repeatedly replacing a non-terminal (initially the start symbol) by the right side of a production, for that non-terminal.
```

# Syntax Analyzers
```
A syntax analyzer or parser takes the input from a lexical analyzer in the form of token streams. 
The parser analyzes the source code (token stream) against the production rules to detect any errors in the code. 
The output of this phase is a parse tree.
```
# Syntax Analyzer
```
This way, the parser accomplishes two tasks, i.e., parsing the code, looking for errors and generating a parse tree as the output of the phase.
```
                        ![SYNTAX2](https://user-images.githubusercontent.com/37740006/75222860-5e4a5d00-57cf-11ea-8f6f-78a0725760a3.png)
```
Parsers are expected to parse the whole code even if some errors exist in the program. Parsers use error recovering strategies, which we will learn later in this chapter.
```
# Derivation
```
A derivation is basically a sequence of production rules, in order to get the input string. 
During parsing, we take two decisions for some sentential form of input:
```
```
Deciding the non-terminal which is to be replaced.
Deciding the production rule, by which, the non-terminal will be replaced.
To decide which non-terminal to be replaced with production rule, we can have two options.
```
# Left-most Derivation
```
If the sentential form of an input is scanned and replaced from left to right, 
it is called left-most derivation. 
The sentential form derived by the left-most derivation is called the left-sentential form.
```
# Right-most Derivation
```
If we scan and replace the input with production rules, from right to left, it is known as right-most derivation. 
The sentential form derived from the right-most derivation is called the right-sentential form.
```

# Example
```
Production rules:

E → E + E
E → E * E
E → id 
Input string: id + id * id
```
# The left-most derivation is:
```
E → E * E
E → E + E * E
E → id + E * E
E → id + id * E
E → id + id * id
Notice that the left-most side non-terminal is always processed first.
```
# The right-most derivation is:
```
E → E + E
E → E + E * E
E → E + E * id
E → E + id * id
E → id + id * id
```
# Parse Tree
```
A parse tree is a graphical depiction of a derivation. 
It is convenient to see how strings are derived from the start symbol. 
The start symbol of the derivation becomes the root of the parse tree. 
Let us see this by an example from the last topic.
```
# We take the left-most derivation of a + b * c
```
The left-most derivation is:

E → E * E
E → E + E * E
E → id + E * E
E → id + id * E
E → id + id * id
```

            ![STEP 1AND2](https://user-images.githubusercontent.com/37740006/75222955-9ea9db00-57cf-11ea-87ff-39b1e66d41dc.png)
            E → E * E	Parse Tree Construction
            ![345](https://user-images.githubusercontent.com/37740006/75226167-21ce2f80-57d6-11ea-8dc1-90cc2ea13fe2.png)
```
All leaf nodes are terminals.
All interior nodes are non-terminals.
In-order traversal gives original input string.
A parse tree depicts associativity and precedence of operators. 
The deepest sub-tree is traversed first, 
therefore the operator in that sub-tree gets precedence over the operator which is in the parent nodes.
```
# Ambiguity
```
A grammar G is said to be ambiguous if it has more than one parse tree (left or right derivation) for at least one string.
```
```
Example

E → E + E
E → E – E
E → id
For the string id + id – id, the above grammar generates two parse trees:
```
![ambuigity](https://user-images.githubusercontent.com/37740006/75226094-f64b4500-57d5-11ea-906f-9537fe309797.png)
# Parse Tree Construction
```
The language generated by an ambiguous grammar is said to be inherently ambiguous.
Ambiguity in grammar is not good for a compiler construction. 
No method can detect and remove ambiguity automatically, 
but it can be removed by either re-writing the whole grammar without ambiguity, 
or by setting and following associativity and precedence constraints.
```
# Associativity
```
If an operand has operators on both sides, 
the side on which the operator takes this operand is decided by the associativity of those operators. 
If the operation is left-associative, 
then the operand will be taken by the left operator or if the operation is right-associative, 
the right operator will take the operand.
```
# Example
```
Operations such as Addition, Multiplication, Subtraction, and Division are left associative. If the expression contains:

id op id op id
it will be evaluated as:

(id op id) op id
For example, (id + id) + id

Operations like Exponentiation are right associative, i.e., the order of evaluation in the same expression will be:

id op (id op id)
For example, id ^ (id ^ id)
```

# Precedence
```
If two different operators share a common operand, the precedence of operators decides which will take the operand. 
That is, 
            2+3*4 can have two different parse trees, 

            one corresponding to (2+3)*4 and 

            another corresponding to 2+(3*4). 

            By setting precedence among operators, this problem can be easily removed. 
As in the previous example, mathematically * (multiplication) has precedence over + (addition), 
so the expression 2+3*4 will always be interpreted as:
```
## 2 + (3 * 4)
```
These methods decrease the chances of ambiguity in a language or its grammar.
```
